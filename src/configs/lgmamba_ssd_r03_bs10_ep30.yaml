defaults:
  - model: lgmamba_lightfsde
  - writer: wandb
  - metrics: brats23_seg
  - datasets: brats23_mixed_train_cached_val_raw_notest
  - dataloader: brats23
  - transforms: brats23_mixed_cached_train_raw_eval
  - _self_

optimizer:
  _target_: torch.optim.AdamW
  lr: 8e-5
  weight_decay: 1e-5

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.n_epochs}

loss_function:
  _target_: src.loss.DiceFocalSegLoss
  dice_weight: 1.0
  focal_weight: 1.0
  gamma: 2.0
  alpha: null

writer:
  mode: offline
  run_name: lgmamba_ssd_r03_bs10_ep30
  loss_names: ["loss", "loss_dice", "loss_focal", "loss_ds"]

dataloader:
  batch_size: 10
  eval_batch_size: 1
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

datasets:
  train:
    cache_dir: "/root/reExp/data/cached/brats23_no_n4"
    usage_ratio: 0.3
  val:
    data_dir: "/root/reExp/data/BraTS2023"
    usage_ratio: 0.3
    cache_in_memory: true

trainer:
  log_step: 10
  n_epochs: 30
  eval_partitions: ["val"]
  ddp:
    enabled: false
    backend: nccl
    find_unused_parameters: false
    distributed_eval: false
  lr_scheduler_step_per: epoch
  warmup:
    enabled: true
    epochs: 3
    start_factor: 0.1
    end_factor: 1.0
  amp: true
  amp_dtype: bf16
  et_threshold_search:
    enabled: false
    parts: ["val"]
    channel_index: 2
    thresholds: [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]
    apply_sigmoid: true
    smooth: 1e-5
    log_all: false
  epoch_len: null
  use_sliding_window_inference: true
  sw_roi_size: [96, 96, 96]
  sw_batch_size: 1
  sw_overlap: 0.5
  device_tensors: ["image", "label"]
  resume_from: null
  device: auto
  override: true
  monitor: "max val_MeanDice"
  save_period: 10
  early_stop: ${trainer.n_epochs}
  save_dir: "saved"
  seed: 42
