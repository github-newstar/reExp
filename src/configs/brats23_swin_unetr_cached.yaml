defaults:
  - model: swin_unetr_brats23
  - writer: wandb
  - metrics: brats23_seg
  - datasets: brats23_cached
  - dataloader: brats23
  - transforms: brats23_cached
  - _self_

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  weight_decay: 1e-5

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.n_epochs}

loss_function:
  _target_: src.loss.DiceFocalSegLoss
  dice_weight: 1.0
  focal_weight: 1.0
  gamma: 2.0
  alpha: null

writer:
  loss_names: ["loss", "loss_dice", "loss_focal", "loss_ds"]

trainer:
  log_step: 10
  n_epochs: 100
  amp: False
  amp_dtype: bf16
  et_threshold_search:
    enabled: False
    parts: ["val"]
    channel_index: 2
    thresholds: [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]
    apply_sigmoid: True
    smooth: 1e-5
    log_all: False
  epoch_len: null
  use_sliding_window_inference: True
  sw_roi_size: [96, 96, 96]
  sw_batch_size: 1
  sw_overlap: 0.5
  device_tensors: ["image", "label"]
  resume_from: null
  device: auto
  override: False
  monitor: "max val_MeanDice"
  save_period: 10
  early_stop: ${trainer.n_epochs}
  save_dir: "saved"
  seed: 42
