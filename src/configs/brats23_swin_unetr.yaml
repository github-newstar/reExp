defaults:
  - model: swin_unetr_brats23
  - writer: wandb
  - metrics: brats23_seg
  - datasets: brats23
  - dataloader: brats23
  - transforms: brats23
  - _self_

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4
  weight_decay: 1e-5

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${trainer.n_epochs}

loss_function:
  _target_: src.loss.DiceFocalSegLoss
  dice_weight: 1.0
  focal_weight: 1.0
  gamma: 2.0
  alpha: null

writer:
  loss_names: ["loss", "loss_dice", "loss_focal"]

trainer:
  log_step: 10
  n_epochs: 100
  epoch_len: null
  use_sliding_window_inference: True
  sw_roi_size: [96, 96, 96]
  sw_batch_size: 1
  sw_overlap: 0.5
  device_tensors: ["image", "label"]
  resume_from: null
  device: auto
  override: False
  monitor: "max val_MeanDice"
  save_period: 10
  early_stop: ${trainer.n_epochs}
  save_dir: "saved"
  seed: 42
